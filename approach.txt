# TASK: Reproduce the Coconut training curriculum (CoT stage, then latent stage) on a small subset with a tiny model, running entirely on CPU.

# PREPARATION
# Fork https://github.com/facebookresearch/coconut on GitHub (make a personal copy (a fork) under your GitHub account)
# Create a local folder called coconut-cpu-mini containing the forkâ€™s code.
git clone https://github.com/LuciaLicakova/coconut-cpu-mini.git coconut-cpu-mini
cd coconut-cpu-mini
# Do not edit directly on main, go to a branch called cpu-mini.
git checkout -b cpu-mini
# Make and activate a virtual environment
python -m venv .venv
.venv\Scripts\activate.bat
# Install required packages
pip install --upgrade pip
pip install --index-url https://download.pytorch.org/whl/cpu torch torchvision torchaudio
pip install transformers>=4.42 datasets pyyaml tqdm
pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# REPRODUCING EXPERIMENTS
# Preprocess files to have gsm_train.json, gsm_test.json, gsm_valid.json in coconut-cpu-mini/data
PS C:\Users\lucia\AppData\Local\Programs\Python\Python310\coconut-cpu-mini> python preprocessing/prepare_gsm.py 
# Create CPU-friendly configuration files that tell Coconut how to train the tiny model on the GSM8K dataset: cpu_args\gsm_cot_cpu.yaml (stage 0), cpu_args\gsm_coconut_cpu.yaml (stage 1), cpu_args\gsm_coconut_eval_cpu.yaml

# Train on CPU (tiny model) with CoT as stage 0
python run.py cpu_args/gsm_cot_cpu.yaml

# Train using GPU
PS C:\Users\lucia\AppData\Local\Programs\Python\Python310\coconut-cpu-mini> ssh student01
D3a3SeNH
nvidia-smi
# Make sure GPU 1 is free.
# Update the Google Sheet: enter your name (licakova) and the GPU number to reserve it
# Activate environment
source /system/apps/studentenv/miniconda3/bashrc
conda activate coconut
cd /system/user/studentwork/licakova/coconut-cpu-mini
# Start training using tmux (keeps running if you log out)
tmux new -s coconut_run
# Run on a single GPU
CUDA_VISIBLE_DEVICES=1 python run.py cpu_args/gsm_cot_cpu.yaml

# After modifying locally
ctrl+b d
# Leave the ssh session
exit
# Update from local Windows PowerShell
scp -P 5792 C:\Users\lucia\AppData\Local\Programs\Python\Python310\coconut-cpu-mini\run.py licakova@student01.ai-lab.jku.at:/system/user/studentwork/licakova/coconut-cpu-mini/
scp -P 5792 -r C:\Users\lucia\AppData\Local\Programs\Python\Python310\coconut-cpu-mini\cpu_args licakova@student01.ai-lab.jku.at:/system/user/studentwork/licakova/coconut-cpu-mini/

# Attach tmux
ssh -p 5792 licakova@student01.ai-lab.jku.at
tmux attach -t coconut_run
# Run the code




# Once it starts, detach tmux: Ctrl+b then d
# Check progress, GPU usage, CPU/RAM usage anytime
tmux attach -t coconut_run 
nvidia-smi                  
top                         
# When done: kill process if needed
ps aux | grep python
kill <PID>
# Update the Google Sheet to free the GPU.
-----------------------

# With gpus, update yaml settings

# Select a checkpoint as the initialization of Coconut (the validation accuracy is expected to be around 40%). Replace the load_model_path in the args/gsm_coconut.yaml with the selected checkpoint, and run
python run.py cpu_args/gsm_coconut_cpu.yaml


# Find the checkpoint with best validation accuracy, and put the path as load_model_path in args/gsm_coconut_eval.yaml. Then evaluate
python run.py cpu_args/gsm_coconut_eval_cpu.yaml


# In run.py, dist calls are wrapped to avoid errors (when only using CPU), DDP/FSDP is skipped, and .to(rank) is replaced with .to(device)
# train: PS C:\Users\lucia\AppData\Local\Programs\Python\Python310\coconut-cpu-mini> python run.py cpu_args/gsm_cot_cpu.yaml


