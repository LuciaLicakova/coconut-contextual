# TASK: Reproduce the Coconut training curriculum (CoT stage, then latent stage) on a small subset with a tiny model, running entirely on CPU.

# PREPARATION
# Fork https://github.com/facebookresearch/coconut on GitHub (make a personal copy (a fork) under your GitHub account)
# Create a local folder called coconut-cpu-mini containing the forkâ€™s code.
git clone https://github.com/LuciaLicakova/coconut-cpu-mini.git coconut-cpu-mini
cd coconut-cpu-mini
# Do not edit directly on main, go to a branch called cpu-mini.
git checkout -b cpu-mini
# Make and activate a virtual environment
python -m venv .venv
.venv\Scripts\activate.bat
# Install required packages
pip install --upgrade pip
pip install --index-url https://download.pytorch.org/whl/cpu torch torchvision torchaudio
pip install transformers>=4.42 datasets pyyaml tqdm
pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu

# REPRODUCING EXPERIMENTS
# Preprocess files to have gsm_train.json, gsm_test.json, gsm_valid.json in coconut-cpu-mini/data
PS C:\Users\lucia\AppData\Local\Programs\Python\Python310\coconut-cpu-mini> python preprocessing/prepare_gsm.py 
# Create CPU-friendly configuration files that tell Coconut how to train the tiny model on the GSM8K dataset: cpu_args\gsm_cot_cpu.yaml (stage 0), cpu_args\gsm_coconut_cpu.yaml (stage 1), cpu_args\gsm_coconut_eval_cpu.yaml

# MULTIPLE GPUS
PS C:\Users\lucia\AppData\Local\Programs\Python\Python310\coconut-cpu-mini>wandb login
wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)
wandb: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models
wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:
wandb: No netrc file found, creating one.
wandb: Appending key for api.wandb.ai to your netrc file: C:\Users\lucia\_netrc
wandb: Currently logged in as: lucia-licakova (lucia-licakova-johannes-kepler-universit-t-linz) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin

# Train on CPU (tiny model) with CoT as stage 0
python run.py cpu_args/gsm_cot_cpu.yaml

# Train using one GPU
PS C:\Users\lucia\AppData\Local\Programs\Python\Python310\coconut-cpu-mini> ssh student01
D3a3SeNH
nvidia-smi
# Make sure GPU 1 is free.
# Update the Google Sheet: enter your name (licakova) and the GPU number to reserve it
# Activate environment
source /system/apps/studentenv/miniconda3/bashrc
conda activate coconut
cd /system/user/studentwork/licakova/coconut-cpu-mini
# Start training using tmux (keeps running if you log out)
tmux new -s coconut_run
# Run on a single GPU
CUDA_VISIBLE_DEVICES=1 python run.py cpu_args/gsm_cot_cpu.yaml
# 2 GPUs
CUDA_VISIBLE_DEVICES=0,1 torchrun --nnodes=1 --nproc_per_node=2 run.py cpu_args/gsm_cot.yaml


# After modifying locally
ctrl+b d
# Leave the ssh session
exit
# Update from local Windows PowerShell
scp -P 5792 C:\Users\lucia\AppData\Local\Programs\Python\Python310\coconut-cpu-mini\run.py licakova@student01.ai-lab.jku.at:/system/user/studentwork/licakova/coconut-cpu-mini/
scp -P 5792 -r C:\Users\lucia\AppData\Local\Programs\Python\Python310\coconut-cpu-mini\cpu_args licakova@student01.ai-lab.jku.at:/system/user/studentwork/licakova/coconut-cpu-mini/

# Attach tmux
ssh -p 5792 licakova@student01.ai-lab.jku.at
tmux attach -t coconut_run
# Run the code




# Once it starts, detach tmux: Ctrl+b then d
# Check progress, GPU usage, CPU/RAM usage anytime
tmux attach -t coconut_run 
nvidia-smi                  
top                         
# When done: kill process if needed
ps aux | grep python
kill <PID>
# Update the Google Sheet to free the GPU.
-----------------------

# With gpus, update yaml settings

# Select a checkpoint as the initialization of Coconut (the validation accuracy is expected to be around 40%). Replace the load_model_path in the args/gsm_coconut.yaml with the selected checkpoint, and run
python run.py cpu_args/gsm_coconut_cpu.yaml


# Find the checkpoint with best validation accuracy, and put the path as load_model_path in args/gsm_coconut_eval.yaml. Then evaluate
python run.py cpu_args/gsm_coconut_eval_cpu.yaml


# In run.py, dist calls are wrapped to avoid errors (when only using CPU), DDP/FSDP is skipped, and .to(rank) is replaced with .to(device)
# train: PS C:\Users\lucia\AppData\Local\Programs\Python\Python310\coconut-cpu-mini> python run.py cpu_args/gsm_cot_cpu.yaml
# my modified files are in cpu_args although they are also used for GPU training (use *_cpu.yaml for CPU or a single GPU; batch_size_training: 16)

-----------------------------
AFTER CUDA_VISIBLE_DEVICES=1 python run.py cpu_args/gsm_cot_cpu.yaml
[licakova@student01 coconut-cpu-mini]$ tmux capture-pane -S - -E - -p > full_run.log
[licakova@student01 coconut-cpu-mini]$ grep -E "Accuracy on validation set|saving model" full_run.log
Accuracy on validation set: 0 / 32 = 0.0
Accuracy on validation set: 4 / 32 = 0.125
Accuracy on validation set: 2 / 32 = 0.0625
Accuracy on validation set: 0 / 32 = 0.0
Accuracy on validation set: 0 / 32 = 0.0
Accuracy on validation set: 0 / 32 = 0.0
Accuracy on validation set: 1 / 32 = 0.03125
Accuracy on validation set: 1 / 32 = 0.03125
Accuracy on validation set: 1 / 32 = 0.03125
Accuracy on validation set: 0 / 32 = 0.0
Accuracy on validation set: 2 / 32 = 0.0625
Accuracy on validation set: 3 / 32 = 0.09375
Accuracy on validation set: 1 / 32 = 0.03125
Accuracy on validation set: 2 / 32 = 0.0625
Accuracy on validation set: 2 / 32 = 0.0625
Accuracy on validation set: 1 / 32 = 0.03125


