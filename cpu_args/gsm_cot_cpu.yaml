# Stage 0: train the model to generate reasoning steps normally, without using any latent thoughts yet. Produces a “best.pt” checkpoint that Stage 1 (Coconut latent) will later load.
project: gsm8k_cot_cpu
name: run1
# where model checkpoints are saved
save_path: exps/gsm8k_cot_cpu
# train the model on train_path and test on val_path after every epoch
only_eval: false

# Training settings
coconut: False
cot: True
no_thoughts: False
no_cot: False
# number of continuous thoughts for each reasoning step
c_thought: 1
epochs_per_stage: 3
# the maximum number of training stages (in addition to the initial stage)
max_latent_stage: 3
pad_latent_to_max: false
# only save checkpoint if validation improves
save_only_improve: true
uniform_prob: 0.0

# Model
# small enough to run on CPU
model_id: sshleifer/tiny-gpt2
# train from scratch
load_model_path: None

# Misc
seed: 0
# don’t resume from previous checkpoints
resume: 0
# most CPUs don’t have native support for bfloat16 math
bf16: false

# Data (tiny dataset produced by our script)
train_path: data/gsm_train.json
val_path: data/gsm_valid.json

# Optimizer
reset_optimizer: false
batch_size_training: 4
debug: true
# Process 16 single-example batches, sum the gradients, perform one weight update; simulate a batch size of 16 without needing extra RAM
gradient_accumulation_steps: 16
# see the entire training set
num_epochs: 3
lr: 5.0e-5
# regularization term that penalizes large weights to prevent overfitting; not needed for this small CPU run
weight_decay: 0.0
